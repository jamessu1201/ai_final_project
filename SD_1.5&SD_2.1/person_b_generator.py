# code/person_b_generator.py
import torch
from diffusers import StableDiffusionPipeline
from prompts import PROMPTS, NEGATIVE_PROMPT
import time
import json
from pathlib import Path
from datetime import datetime
import traceback

class PersonB_Generator:
    def __init__(self, output_dir="../results/person_b"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.device = "cuda"
        self.results = []
        
        # è¨˜éŒ„GPUè³‡è¨Š
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
    def load_sd15(self):
        """è¼‰å…¥SD 1.5"""
        print("\\nğŸ“¥ Loading Stable Diffusion 1.5...")
        self.sd15_pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16,
            safety_checker=None,  # é—œé–‰safety checkeråŠ é€Ÿ
        ).to(self.device)
        
        # å•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–
        self.sd15_pipe.enable_attention_slicing()
        print("âœ… SD 1.5 loaded!")
        
    def load_sd21(self):
        """è¼‰å…¥SD 2.1"""
        print("\\nğŸ“¥ Loading Stable Diffusion 2.1 (Manojb Version)...")
        self.sd21_pipe = StableDiffusionPipeline.from_pretrained(
            "Manojb/stable-diffusion-2-1-base",
            torch_dtype=torch.float16,
            safety_checker=None,
        ).to(self.device)
        
        self.sd21_pipe.enable_attention_slicing()
        print("âœ… SD 2.1 loaded!")
    
    def generate_image(self, pipe, prompt, negative_prompt=None, seed=42):
        """ç”Ÿæˆå–®å¼µåœ–ç‰‡"""
        generator = torch.Generator(self.device).manual_seed(seed)
        
        # è¨˜éŒ„é–‹å§‹æ™‚é–“å’Œè¨˜æ†¶é«”
        start_time = time.time()
        torch.cuda.reset_peak_memory_stats()
        
        try:
            image = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                guidance_scale=7.5,
                num_inference_steps=30,
                generator=generator,
                height=512,
                width=512,
            ).images[0]
            
            gen_time = time.time() - start_time
            peak_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB
            
            return image, gen_time, peak_memory, True
            
        except Exception as e:
            print(f"âŒ Error generating image: {e}")
            traceback.print_exc()
            return None, 0, 0, False
    
    def experiment_1_sd15_baseline(self):
        """å¯¦é©—1: SD 1.5 åŸºç¤ç‰ˆæœ¬"""
        print("\\n" + "="*60)
        print("ğŸ§ª å¯¦é©—1: SD 1.5 Baseline")
        print("="*60)
        
        self.load_sd15()
        
        for i, prompt in enumerate(PROMPTS):
            print(f"\\n[{i+1}/50] Generating with SD 1.5...")
            print(f"Prompt: {prompt[:50]}...")
            
            image, gen_time, peak_mem, success = self.generate_image(
                self.sd15_pipe, prompt
            )
            
            if not success:
                continue
            
            # å„²å­˜åœ–ç‰‡
            save_path = self.output_dir / f"sd15_baseline_{i:03d}.png"
            image.save(save_path)
            
            # è¨˜éŒ„çµæœ
            result = {
                "experiment": "sd15_baseline",
                "model": "SD-1.5",
                "prompt_id": i,
                "prompt": prompt,
                "negative_prompt": None,
                "generation_time": round(gen_time, 2),
                "peak_memory_gb": round(peak_mem, 2),
                "image_path": str(save_path),
                "timestamp": datetime.now().isoformat(),
            }
            self.results.append(result)
            
            print(f"â±ï¸  Time: {gen_time:.2f}s | ğŸ’¾ Memory: {peak_mem:.2f}GB")
            
            # æ¯10å¼µå„²å­˜ä¸€æ¬¡ï¼ˆé¿å…crashæå¤±è³‡æ–™ï¼‰
            if (i + 1) % 10 == 0:
                self.save_results()
                print(f"ğŸ’¾ Progress saved! ({i+1}/50)")
        
        print("\\nâœ… å¯¦é©—1å®Œæˆï¼")
    
    def experiment_2_sd15_negative(self):
        """å¯¦é©—2: SD 1.5 + Negative Prompt"""
        print("\\n" + "="*60)
        print("ğŸ§ª å¯¦é©—2: SD 1.5 + Negative Prompt")
        print("="*60)
        
        # SD 1.5å·²ç¶“è¼‰å…¥ï¼Œç›´æ¥ä½¿ç”¨
        
        for i, prompt in enumerate(PROMPTS):
            print(f"\\n[{i+1}/50] Generating with SD 1.5 + Negative...")
            print(f"Prompt: {prompt[:50]}...")
            
            image, gen_time, peak_mem, success = self.generate_image(
                self.sd15_pipe, prompt, negative_prompt=NEGATIVE_PROMPT
            )
            
            if not success:
                continue
            
            save_path = self.output_dir / f"sd15_negative_{i:03d}.png"
            image.save(save_path)
            
            result = {
                "experiment": "sd15_negative",
                "model": "SD-1.5",
                "prompt_id": i,
                "prompt": prompt,
                "negative_prompt": NEGATIVE_PROMPT,
                "generation_time": round(gen_time, 2),
                "peak_memory_gb": round(peak_mem, 2),
                "image_path": str(save_path),
                "timestamp": datetime.now().isoformat(),
            }
            self.results.append(result)
            
            print(f"â±ï¸  Time: {gen_time:.2f}s | ğŸ’¾ Memory: {peak_mem:.2f}GB")
            
            if (i + 1) % 10 == 0:
                self.save_results()
                print(f"ğŸ’¾ Progress saved! ({i+1}/50)")
        
        print("\\nâœ… å¯¦é©—2å®Œæˆï¼")
        
        # é‡‹æ”¾SD 1.5è¨˜æ†¶é«”
        del self.sd15_pipe
        torch.cuda.empty_cache()
        print("ğŸ—‘ï¸  SD 1.5 unloaded from memory")
    
    def experiment_3_sd21_baseline(self):
        """å¯¦é©—3: SD 2.1 åŸºç¤ç‰ˆæœ¬"""
        print("\\n" + "="*60)
        print("ğŸ§ª å¯¦é©—3: SD 2.1 Baseline")
        print("="*60)
        
        self.load_sd21()
        
        for i, prompt in enumerate(PROMPTS):
            print(f"\\n[{i+1}/50] Generating with SD 2.1...")
            print(f"Prompt: {prompt[:50]}...")
            
            image, gen_time, peak_mem, success = self.generate_image(
                self.sd21_pipe, prompt
            )
            
            if not success:
                continue
            
            save_path = self.output_dir / f"sd21_baseline_{i:03d}.png"
            image.save(save_path)
            
            result = {
                "experiment": "sd21_baseline",
                "model": "SD-2.1",
                "prompt_id": i,
                "prompt": prompt,
                "negative_prompt": None,
                "generation_time": round(gen_time, 2),
                "peak_memory_gb": round(peak_mem, 2),
                "image_path": str(save_path),
                "timestamp": datetime.now().isoformat(),
            }
            self.results.append(result)
            
            print(f"â±ï¸  Time: {gen_time:.2f}s | ğŸ’¾ Memory: {peak_mem:.2f}GB")
            
            if (i + 1) % 10 == 0:
                self.save_results()
                print(f"ğŸ’¾ Progress saved! ({i+1}/50)")
        
        print("\\nâœ… å¯¦é©—3å®Œæˆï¼")
        
        del self.sd21_pipe
        torch.cuda.empty_cache()
    
    def save_results(self):
        """å„²å­˜çµæœåˆ°JSON"""
        with open(self.output_dir / "results.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
    
    def run_all_experiments(self):
        """åŸ·è¡Œæ‰€æœ‰å¯¦é©—"""
        start_time = time.time()
        
        print("\\n" + "="*60)
        print("ğŸš€ Person B - SD Series Comparison")
        print("="*60)
        
        try:
            # å¯¦é©—1: SD 1.5 baseline
            self.experiment_1_sd15_baseline()
            
            # å¯¦é©—2: SD 1.5 + negative prompt
            self.experiment_2_sd15_negative()
            
            # å¯¦é©—3: SD 2.1 baseline
            self.experiment_3_sd21_baseline()
            
            # æœ€çµ‚å„²å­˜
            self.save_results()
            
            total_time = time.time() - start_time
            print("\\n" + "="*60)
            print(f"ğŸ‰ æ‰€æœ‰å¯¦é©—å®Œæˆï¼")
            print(f"â±ï¸  ç¸½è€—æ™‚: {total_time/3600:.2f} å°æ™‚")
            print(f"ğŸ“Š ç¸½å…±ç”Ÿæˆ: {len(self.results)} å¼µåœ–ç‰‡")
            print(f"ğŸ’¾ çµæœå„²å­˜åœ¨: {self.output_dir}")
            print("="*60)
            
        except Exception as e:
            print(f"\\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()
            self.save_results()  # å³ä½¿å‡ºéŒ¯ä¹Ÿå„²å­˜å·²å®Œæˆçš„éƒ¨åˆ†

if __name__ == "__main__":
    generator = PersonB_Generator()
    generator.run_all_experiments()